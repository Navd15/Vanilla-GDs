{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Vailla implementation of Gradient Descent to minimize MSE from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loss_fn(theta,y,x,idx=-1):\n",
    "    \"\"\"\n",
    "    Defines a loss (h(theta.x)-y)*2 over single sample\n",
    "\n",
    "    Parameters:\n",
    "        theta (list or numpy array) 1-D: The weights for the function\n",
    "        y (list or numpy array) 1-D: Truth values\n",
    "        x (2-D list or numpy array): samples (x_i)\n",
    "        idx (int) default=-1: index of theta. if other than -1 that means _loss_fn called for derivative. \n",
    "    Returns:\n",
    "        float\n",
    "    \"\"\"\n",
    "    if idx>=0:\n",
    "        return (np.matmul(theta,x) - y) \n",
    "    return (np.matmul(theta,x) - y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cost_fn(theta,y,x,theta_idx=-1)-> float:\n",
    "    \"\"\"\n",
    "    Defines cost MSE over a dataset x\n",
    "\n",
    "    Parameters:\n",
    "        theta (list or numpy array) 1-D: The weights for the function\n",
    "        y (list or numpy array) 1-D: Truth values\n",
    "        x (2-D list or numpy array): samples (x_i)\n",
    "        theta_idx (int) default=-1: index of theta. if other than -1 that means _cost_fn called for derivative. \n",
    "    Returns:\n",
    "        summation (float): total cost over the dataset\n",
    "    \"\"\"\n",
    "    rows=len(x)\n",
    "    summation=0\n",
    "    for i,j in enumerate(x):\n",
    "        #  if greater than -1 then calculate the derivative wrt to theta[theta_idx]\n",
    "         if theta_idx>=0:\n",
    "            summation+=_loss_fn(theta,y[i],x[i],theta_idx)*x[i][theta_idx]\n",
    "         else:\n",
    "            summation+=_loss_fn(theta,y[i],x[i]) \n",
    "    if theta_idx>=0:\n",
    "        return summation/rows\n",
    "    # divide by 2 if not called for derivative\n",
    "    return summation/(2*rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_fn(y,x,alpha):\n",
    "    \"\"\"\n",
    "    minimize MSE cost and updates global theta\n",
    "\n",
    "    Parameters:\n",
    "        y (list or numpy array) 1-D: Truth values\n",
    "        x (list or numpy array) 2-D: dataset.\n",
    "        alpha (float): learning rate\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # initialize theta as zeros with dimensions x+1 for bias/intercept\n",
    "    theta=[0]*(1+len(x[0]))\n",
    "    # add x_0=1 to all the datapoints \n",
    "    x=np.hstack(([[1]]*len(x),x))\n",
    "    change=math.inf\n",
    "    # calculate th\n",
    "    old_cost=_cost_fn(theta,y,x)\n",
    "    \n",
    "    latest_cost=math.inf\n",
    "    # infinite loop\n",
    "    while True:\n",
    "        # if latest_cost==math.inf then its first time; dont check change\n",
    "        if latest_cost!=math.inf:\n",
    "            # as the cost is not changing very much we have reached a miniumum.\n",
    "            if change<0.000001:\n",
    "                break\n",
    "            # update the old cost with latest cost calculated with new weights\n",
    "            old_cost=latest_cost\n",
    "        '''\n",
    "        generate new theta into temp_theta \n",
    "        but we dont update until all the thetas are updated in a single loop\n",
    "        '''\n",
    "        temp_theta=[0]*len(theta)\n",
    "        for i,j in enumerate(theta):\n",
    "            temp_theta[i]=theta[i]- (alpha * _cost_fn(theta,y,x,i))\n",
    "        # update theta when an epoch is done\n",
    "        theta=temp_theta\n",
    "        # calculate the new cost with new weights\n",
    "        latest_cost=_cost_fn(theta,y,x)\n",
    "        # calculate the change \n",
    "        change=old_cost-latest_cost\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=make_regression(10,n_features=2,n_targets=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08393925238986102, 84.13068452614661, 83.45419453798235]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_fn(y,x,0.0001)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
